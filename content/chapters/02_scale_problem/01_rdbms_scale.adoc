== Scaling, Old School

When I got interested in Cassandra, I was working at some startups that had tried to build scalable systems with tight SLAs and had run into some trouble.  This was a point in time when using a relational database and vertical scaling was the tried and true solution.

Once we hit the limits of vertical scaling, the next step was to chip away at the advantages of the RDBMS.

=== Caching

The first (and easiest) problem that needed to be solved was dealing with read heavy workload.  Fortunately, solving reads is usually solved by making more copies of the data and using more servers, in the form of caching.

Caching servers are easily scaled by partitioning the data among all the servers.  Splitting data among caching servers is easy.   This works great for read heavy workloads and scales linearly for a long time.

You've probably heard of one of the most well known caching projects of all time, Memcached.  Memcached made it possible to massively scale read heavy workloads by exposing a simple api.  The Memcached api only exposes a simple key/value api for setting and retrieving data.  The following (crappy) code shows the basic logic:

```python
def get_something(id):
    tmp = memcached.get(key)
    if(tmp):
        return tmp
    result = db.query("SELECT * from stuff WHERE id = " + id)
    memcached.put(key, result)
    return result
```

This is fine for small objects / rows, but doesn't work so well with large datasets that are joined and aggregated.  How do we know what cache items to expire when a single row changes?  We still have a problem with more complex workloads.

As the system grows and our SLAs remain the same, we have to start using optimizations to avoid performing the costly operations.  Instead of using expensive joins, we denormalize and copy data around to avoid the costly join.  Instead of aggregating on the fly we precalculate statistics.  We can skip grouping hundreds of thousands (or millions) and return a handful of rows, which can be orders of magnitude faster than trying to do the calculation on the fly.

=== Scaling Writes

When it comes to scaling reads, this is a proven path.  The difficulty arises when we have more work than the server is capable of handling.  Scaling vertically works to a point, but gets very expensive and migrations are risky.  Since we've already decoupled the tables from one another by eliminating joins, the next logical step is to start separating the tables.  First we move individual tables off, then usually the tables get sharded across many servers.  Lots of tooling gets written, https://eng.uber.com/mysql-migration/[wheels reinvented].

We start thinking about partitioning our data at this point.  Without joins, we can move tables to different machines without much problem.  We're able to keep scaling our application at the cost of losing the ability to do arbitrary queries.  It's a necessary tradeoff if we're to keep growing.


=== High Availability

The final challenge of scaling is availability.  All of the changes I've described above are designed to help scale the database by spreading the workload from one server to many.  By eliminating costly queries over large data sets we are able to keep our query times constant, and stick to our SLA.  To maximize uptime, we need to replicate our data to multiple servers.  Fortunately replication has been around for a while, so this is pretty easy to


=== Global Scale

One of the most difficult challenges that needs to be solved is working at a global (and possibly planetary) scale.  Concurrency has always been a difficult problem to tackle.

Locks are one way of ensuring writes don't conflict, but the problem gets harder as datacenters become further and further apart.  We don't usually need to think about the limitations of the speed of light, but when trying to coordinate potentially conflicting updates to individual rows in a database we need to start considering the round trip costs.  


